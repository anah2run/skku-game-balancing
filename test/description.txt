  """
    Description:
        The agent (a car) is started at the bottom of a valley. For any given
        state the agent may choose to accelerate to the left, right or cease
        any acceleration.
    Source:
        The environment appeared first in Andrew Moore's PhD Thesis (1990).
    Observation:
        Type: Box(2)
        Num    Observation               Min            Max
        0      Car Position              -1.2           0.6
        1      Car Velocity              -0.07          0.07
    Actions:
        Type: Discrete(3)
        Num    Action
        0      Accelerate to the Left
        1      Don't accelerate
        2      Accelerate to the Right
        Note: This does not affect the amount of velocity affected by the
        gravitational pull acting on the car.
    Reward:
         Reward of 0 is awarded if the agent reached the flag (position = 0.5)
         on top of the mountain.
         Reward of -1 is awarded if the position of the agent is less than 0.5.
    Starting State:
         The position of the car is assigned a uniform random value in
         [-0.6 , -0.4].
         The starting velocity of the car is always assigned to 0.
    Episode Termination:
         The car position is more than 0.5
         Episode length is greater than 200
    """

    import math
import sys

import numpy as np

import gym
from gym import spaces
from gym.utils import seeding


class AgentEnv(gym.Env):
    metadata = {"render.modes": ["human", "ansi"]}

    def __init__(self, params, cdt):
        self.viewer = None
        self.seed()
        self.action_space = spaces.Discrete(3)  # wait, skill1, skill2
        self.low = np.array([0] * 6, dtype=np.float32)  # my_hp, my_mp, enemy_hp, enemy_mp, skill1_timer, skill2_timer
        self.high = np.array([1] * 6, dtype=np.float32)
        self.observation_space = spaces.Box(self.low, self.high, dtype=np.float32)
        self.buff_q = {}
        self.max_hp, self.max_mp, self.armor, self.str, self.crit, self.atk_spd, self.reg_hp, self.reg_mp = params
        self.skill_cool = [x for x in cdt]

        self.reset()

    def seed(self, seed=None):
        self.np_random, seed = seeding.np_random(seed)
        return [seed]

    def step(self, action):
        assert self.action_space.contains(action), "%r (%s) invalid" % (
            action,
            type(action),
        )
        self.skill_timer = np.add(self.skill_timer, -0.5)
        self.skill_timer = np.clip(self.skill_timer, 0, self.skill_cool)
        hp_ratio, self.pos, enemy_hp_ratio, self.enemy_pos, _, _, _ = self.state

        done = bool(hp_ratio < 0 or enemy_hp_ratio < 0)
        reward = 0
        # self.state = (hp_ratio, pos, enemy_hp_ratio, enemy_pos, 0, 0, 0)
        return np.array(self.state, dtype=np.float32), reward, done, {}

    def reset(self):
        # self.np_random.uniform(low=-0.6, high=0.6)
        self.hp = self.max_hp
        self.mp = self.max_mp
        self.skill_timer = [0, 0]
        self.state = np.array([1, self.init_pos, 1, enemy_pos, 1, 1, 1])
        return np.array(self.state, dtype=np.float32)

    def render(self, mode="human"):
        return

    def get_keys_to_action(self):
        # Control with left and right arrow keys. example {(): 1, (276,): 0, (275,): 2, (275, 276): 1}
        return {}

    def use_skill(self, id, dist=0):
        if self.skill_timer[id] <= 0 and (self.skill_set[2] <= 0 or self.skill_set[2] >= dist):
            self.skill_timer[id] = self.skill_cool[id]
            return True
        return False

    def close(self):
        if self.viewer:
            self.viewer.close()
            self.viewer = None
